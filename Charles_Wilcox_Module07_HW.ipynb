{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iHK-G76bp0zb"
      },
      "source": [
        "## Homework Week 7\n",
        "\n",
        "1. Why the holdout method for model selection suggests to separate the data into three parts: a training set, a validation set, and a test set?\n",
        "2. Given a data set (wine), split data (20% test), apply pipeline to standardize the data, classify the data using KNeighborsClassifier (n_neighbors=10), print the test accuracy.\n",
        "\n",
        "```python\n",
        "from sklearn import datasets\n",
        "df = datasets.load_wine()\n",
        "X = df.data\n",
        "y = df.target\n",
        "```\n",
        "\n",
        "3. What is learning curve? Base on learning curve, how do you know if the model is over fitting or not?\n",
        "4. In the above data set, fit KNN using 10-fold cross validation and grid search to optimize the number of neighbors; print the optimized parameters and the test accuracy.\n",
        "5. Calculate the accuracy, precision and recall based on the following confusion matrix.\n",
        "\n",
        "|  | predicted N0 | predicted Yes|\n",
        "|--|--------------| -------------|\n",
        "|Actual No| 50 | 10|\n",
        "|Actual Yes| 5 | 100|\n",
        "\n",
        "6. Read the last section in the Chapter 6 of textbook, \"Dealing with class imbalance\". Discuss why the accuracy is not a valid meature metric in an imbalanced dataset? What other metrics can be used then?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qD3P_RnGqHjC"
      },
      "source": [
        "**Problem 1: Why the holdout method for model selection suggests to separate the data into three parts: a training set, a validation set, and a test set?**\n",
        "\n",
        "The Holdout Method suggests that we separate the data into three parts: the training set, the validation set, and the test set. The training set is used to fit different models. The validation set is used to repeatedly assess and evaluate the performance of the models and tune hyperparameters. After assessing each model, one is selected. The performance of the final model can now be assessed using the test set. This test set represents un-seen data and can give us an idea how the final model performs against unknown data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JfbsZw83sFsl"
      },
      "source": [
        "**Problem 2: Given a data set (wine), split data (20% test), apply pipeline to standardize the data, classify the data using KNeighborsClassifier (n_neighbors=10), print the test accuracy.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "V1DpCcQasQ7M"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "df = datasets.load_wine()\n",
        "X = df.data\n",
        "y = df.target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "C3cjE_y4t0ej"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#split the data into 80% training set and 20% test set\n",
        "X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(X, y, \n",
        "                     test_size=0.20,\n",
        "                     stratify=y,\n",
        "                     random_state=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwKFraYyt6m5",
        "outputId": "bca8c08e-1619-4cf8-db4c-53d05cd1f856"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.944\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "#pipeline to standardize the data and apply KNeighborsClassifier\n",
        "pipe = make_pipeline(StandardScaler(),\n",
        "                        KNeighborsClassifier(n_neighbors=10))\n",
        "\n",
        "#fit, predict, and print the accuracy of the KNeighborsClassifier\n",
        "pipe.fit(X_train, y_train)\n",
        "y_pred = pipe.predict(X_test)\n",
        "print('Test Accuracy: %.3f' % pipe.score(X_test, y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqWGAHHeyH9p"
      },
      "source": [
        "**Problem 3: What is learning curve? Base on learning curve, how do you know if the model is over fitting or not?**\n",
        "\n",
        "The learning curve is a plot of the models training and validation accuracies as a fuction of the data size. It is an easy way to detect if the model suffers from overfitting (high variance) or underfitting (high bias). A large gap between the plot of the training accuracy and the validation accuracy is indicative of overfitting (high variance). Conversely, if the plot of the training accuracy and the validation accuracy are low, it is indicative of underfitting (high bias)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VwzU4dnT44dJ"
      },
      "source": [
        "**Problem 4: In the above data set, fit KNN using 10-fold cross validation and grid search to optimize the number of neighbors; print the optimized parameters and the test accuracy.**\n",
        "\n",
        "The follow code is a grid search using just the n_neighbors parameter to find the best accuracy and best number of neighbors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1NXQW0PQeIb",
        "outputId": "fb0c9df3-47b9-4fe4-b918-a84d25eda088"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Accuracy: 0.971\n",
            "Optimized Number of Neighbors: 23.000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_range = [i for i in range(1,101)] # numbers from 1 to 100 for knn clusters\n",
        "\n",
        "param_grid = [{'kneighborsclassifier__n_neighbors': param_range}] # parameter grid with just the n_neighbor hyperparameter\n",
        "\n",
        "# grid search\n",
        "gs = GridSearchCV(estimator=pipe, \n",
        "                  param_grid=param_grid, \n",
        "                  scoring='accuracy',\n",
        "                  cv=10, # 10-fold cross validation\n",
        "                  n_jobs=-1)\n",
        "\n",
        "gs = gs.fit(X_train, y_train) # fit the model\n",
        "\n",
        "print(\"Best Accuracy: %.3f\" % gs.best_score_) # print best accuracy\n",
        "for key, value in gs.best_params_.items():\n",
        "  print(\"Optimized Number of Neighbors: %.3f\" % value) #print optimized number of neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAvF8DzJK2ez"
      },
      "source": [
        "The next block of code adds the hyperparameters 'weights' and 'algorithm' into the grid search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MMkRVWTI9jMD",
        "outputId": "eded5401-66c9-4e88-ff09-427a8f189ec6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best Accuracy: 0.979\n",
            "Best Parameters:\n",
            "kneighborsclassifier__algorithm : ball_tree\n",
            "kneighborsclassifier__n_neighbors : 41\n",
            "kneighborsclassifier__weights : distance\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "param_range = [i for i in range(1,101)]\n",
        "\n",
        "params = {'kneighborsclassifier__n_neighbors': param_range,\n",
        "              'kneighborsclassifier__weights': ('uniform', 'distance'),\n",
        "              'kneighborsclassifier__algorithm': ('ball_tree', 'kd_tree', 'brute')}\n",
        "\n",
        "gs = GridSearchCV(estimator=pipe, \n",
        "                  param_grid=params, \n",
        "                  scoring='accuracy',\n",
        "                  cv=10,\n",
        "                  n_jobs=-1)\n",
        "\n",
        "gs = gs.fit(X_train, y_train)\n",
        "print(\"Best Accuracy: %.3f\" % gs.best_score_)\n",
        "print(\"Best Parameters:\")\n",
        "for key, value in gs.best_params_.items():\n",
        "    print(key, ':', value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-HR5zFGIMgzd"
      },
      "source": [
        "**Problem 5: Calculate the accuracy, precision and recall based on the following confusion matrix.**\n",
        "\n",
        "|  | predicted N0 | predicted Yes|\n",
        "|--|--------------| -------------|\n",
        "|Actual No| 50 | 10|\n",
        "|Actual Yes| 5 | 100|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAwgRuRvNISX"
      },
      "source": [
        "**Accuracy** = (True Yes + True No) / Total = (100 + 50) / 165 = 0.91\n",
        "\n",
        "**Precision** = True Yes / Predicted Yes = 100 / 110 = 0.91\n",
        "\n",
        "**Recall** = True Yes / (True Yes + False No) 100 / (100 + 5) = 0.95"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ri62o9CdQ3zh"
      },
      "source": [
        "**Problem 6: Read the last section in the Chapter 6 of textbook, \"Dealing with class imbalance\". Discuss why the accuracy is not a valid meature metric in an imbalanced dataset? What other metrics can be used then?**\n",
        "\n",
        "In an imbalanced data set, accuracy is not valid metric to assess the predictive power of the model. To use an example from my final project in a previous class, I was dealing with a data set where the target variable was 10% 'Yes - purchased the item' and 90% 'No - did not purchase'. The model predicted mostly Nos and had a 91% accuracy, but I was looking to predict the Yeses and the model was very poor at predicting the Yeses. It predicted almost zero Yeses. The accuracy looked great, but the model was not answering the question I needed it to: will the member make a purchase?\n",
        "\n",
        "Recall is another metric that can provide insight when accuracy is not applicable. In my model, recall was 0.069. Less than 7% of the true yeses were predicted correctly.\n",
        "\n",
        "There are a number of ways to deal with this issue: \n",
        "1. You can assign a larger penalty to wrong predictions. \n",
        "2. You can upsample the minority variable or downsample the majority variable. In the example from my project, I ended up downsampling the majority varibable to create a smaller dataset where the target variable was 50% Nos and 50% Yeses."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
