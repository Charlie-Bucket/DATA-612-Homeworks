{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtTxMnv6oC36"
      },
      "source": [
        "## Homework Week 4\n",
        "\n",
        "1. Summarize the basic idea of logistic regression, support vector machines (SVM), and decision tree.\n",
        "1. Read the section of Random Forest, and summarize the major steps.\n",
        "1. Why do we split samples into training and test set? What does the stratify mean?\n",
        "1. Explain what are the bias-variance trade off and regularization.\n",
        "1. When do we use SVM's kernel method?\n",
        "1. Load the scikit-learn's Wine recognition dataset; separate 20% data as test data set; predict the wine quality using the 3 methods (logistic regression, support vector machines, decision tree), and print the accuracy in the test set of each method. \n",
        "\n",
        "\n",
        "To load the data set:\n",
        "```python\n",
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "\n",
        "df = datasets.load_wine()\n",
        "\n",
        "X = df.data\n",
        "y = df.target\n",
        "\n",
        "```\n",
        "\n",
        "You can check the targets and features by:\n",
        "```python\n",
        "print(df.target_names)\n",
        "print(df.feature_names)\n",
        "```\n",
        "\n",
        "To get more details of the wine data set:https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VsW0iMeDd6cH"
      },
      "source": [
        "**1. Summarize the basic idea of logistic regression and decision tree.**\n",
        "\n",
        "**Logistic Regression**\n",
        "\n",
        "Logistic Regression is a linear model for binary classification. It works very well with linearly separable classes, and can be extended to multiclass classification (OvR, or one-versus-rest method). Logistic Regression predicts the probability that a certain sample belongs to a particualr class. \n",
        "\n",
        "**Decision Tree**\n",
        "\n",
        "Decision Trees are classification models for values that are nonlinearly separable. The data is split based on a series of questions until the the nodes are pure, meaning they are all of the same class. The tree can be pruned by setting limits for maximum depth."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CtGj3jJj9Gg"
      },
      "source": [
        "**2. Read the section of Random Forest, and summarize the major steps.**\n",
        "\n",
        "Since individual decision trees suffer from high variance, Random forests can be implemented to build a more robust model with better performance using the average of a multiple decision trees.\n",
        "\n",
        "**Step 1:** draw a random bootstrap sample of size *n*. Bootstrapping refers to a resampling method where *n* samples are selected from the training set with replacement.\n",
        "\n",
        "**Step 2:** Grow a decision tree from the bootstrap sample. At each node:\n",
        "  \n",
        "> a. randomly select *d* features without replacement.\n",
        "\n",
        "\n",
        "> b. Split the node using the feature that provides the best split accoiring to the objective function, i.e. maximizing the IG\n",
        "\n",
        "**Step 3:** Repeat steps 1 through 2 *k* times. Typically, the larger *k* is, the better the performance of the Random Forest is, but at the expense of an increase in computational cost.\n",
        "\n",
        "**Step 4:** Aggregate the prediction by each tree to assign the class label.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPQ3ig97on0D"
      },
      "source": [
        "**3. Why do we split samples into training and test set? What does the stratify mean?**\n",
        "\n",
        "We split samples into training and test sets because we use the training data to build (or train) the model, but we need unseen/unknown data to evaluate the models performance. If we don't have a test set, there is no way to know how well our model is at predicting unknown variables. The training set is known, and used to build the model. The test set is used to evaluate the model.\n",
        "\n",
        "Stratification is when we split the data into a training and test set, but the data is split so that the training and test sets have the same proportion of class labels as the dataset. Stratified samples are used when the proportions of class labels are very different, or imbalanced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XngIFtk5p85G"
      },
      "source": [
        "**4. Explain what are the bias-variance trade off and regularization.**\n",
        "\n",
        "Bias measures how far the predictions are from the correct values.\n",
        "Variance measurues the variability of the model predictions.\n",
        "\n",
        "A simple model with few parameters may have a high bias and low variance, but a very complex model with many parameters may have a high variance and low bias. A simple model may suffer from underfitting while a complex model may suffer from overfitting. \n",
        "\n",
        "Regularization can be used to prevent overfitting. Regularization is a useful tool to handle a high correlation among features and filter out the noise. It introduces additional bias to penalize extreme parameter values. For instance, a very complex model suffers from high variance and low bias. Introducing additional bias is a way of finding a good bias-variance tradeoff."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k38gknCzt_Nc"
      },
      "source": [
        "**6. Load the scikit-learn's Wine recognition dataset; separate 20% data as test data set; predict the wine quality using the 3 methods (logistic regression, support vector machines, decision tree), and print the accuracy in the test set of each method.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "wHtkJGhLuGJY"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "\n",
        "df = datasets.load_wine() #this is the dataset\n",
        "\n",
        "X = df.data #features\n",
        "y = df.target #targets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "hidden": true,
        "id": "pP96aiytmmjy"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#separate 20% of the data as a test data set\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.2, \n",
        "    random_state=1,\n",
        "    stratify=y\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "-shVWa9GvBjO"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#logistic regression model\n",
        "lr = LogisticRegression(random_state=1, solver='liblinear', multi_class='auto')\n",
        "\n",
        "#fit the logistic regression model to the training data\n",
        "lr.fit(X_train, y_train)\n",
        "\n",
        "#use the test data to predict the test target\n",
        "y_pred = lr.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83IAs8fBxCia",
        "outputId": "520c28f4-c07f-4d81-d198-b7a891f7f281"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.94\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#find the accuracy using the target values in the test set versus the predicted target values\n",
        "print('Accuracy: %.2f' % accuracy_score(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "491KrGWSyDeo"
      },
      "source": [
        "The accuracy of the logistic regression method was 94%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "JexOWMs2yOH6"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "#decision tree model with Gini Impurity split criteria and a max depth of 4\n",
        "tree = DecisionTreeClassifier(criterion='gini', max_depth=4, random_state=1)\n",
        "\n",
        "#fit the decision tree to the training data\n",
        "tree.fit(X_train, y_train)\n",
        "\n",
        "#use the test data to predict the test target\n",
        "tree.y_pred = tree.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlalmr4-yyrW",
        "outputId": "c11498ee-dbf9-4b83-8759-f50f5f61c183"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.97\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "#find the accuracy using the target values in the test set versus the predicted target values\n",
        "print('Accuracy: %.2f' % accuracy_score(y_test, tree.y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdvppJOFzDCW"
      },
      "source": [
        "The accuracy of the decision tree method was 97%."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
